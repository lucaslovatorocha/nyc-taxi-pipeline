{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a5cfc0-837a-463b-93cc-db70abee62cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC # NYC Taxi Pipeline - Databricks Workflows Orchestration\n",
    "# MAGIC ### Stack Tecnologias - Desafio Técnico\n",
    "# MAGIC \n",
    "# MAGIC **Objetivo**: Criar pipeline orquestrado usando Databricks Workflows\n",
    "# MAGIC \n",
    "# MAGIC **Componentes:**\n",
    "# MAGIC 1. Job de ingestão Bronze\n",
    "# MAGIC 2. Job de transformação Silver\n",
    "# MAGIC 3. Job de agregação Gold\n",
    "# MAGIC 4. Monitoramento e alertas\n",
    "# MAGIC 5. Configuração de schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c157fec2-9f05-4c9b-b8e1-c2c2d4d5e229",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD27 Configurações do Workflow definidas\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurações do Workflow\n",
    "workflow_config = {\n",
    "    \"name\": \"NYC_Taxi_Pipeline_Production\",\n",
    "    \"description\": \"Pipeline completo Bronze → Silver → Gold para dados NYC Taxi\",\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"timeout_seconds\": 7200,  # 2 horas\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"lucaslovatotech@gmail.com\"],\n",
    "        \"on_success\": [\"lucaslovatotech@gmail.com\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\uD83D\uDD27 Configurações do Workflow definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6392a07-d3d3-479c-bb58-38c2ca616a92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCB Tasks do pipeline definidas:\n  ✅ bronze_ingestion: Ingestão de dados raw para camada Bronze\n  ✅ silver_transformation: Transformação Bronze → Silver com limpeza\n  ✅ gold_aggregation: Agregações Silver → Gold para análises\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# Definir tasks do pipeline\n",
    "pipeline_tasks = {\n",
    "    \"task_1_bronze_ingestion\": {\n",
    "        \"task_key\": \"bronze_ingestion\",\n",
    "        \"description\": \"Ingestão de dados raw para camada Bronze\",\n",
    "        \"notebook_task\": {\n",
    "            \"notebook_path\": \"/Workspace/Users/lucaslovatorocha1@gmail.com/stack-tecnico/notebooks/01_bronze_ingestion\",\n",
    "            \"base_parameters\": {\n",
    "                \"environment\": \"production\",\n",
    "                \"date\": \"{{ ds }}\"\n",
    "            }\n",
    "        },\n",
    "        \"new_cluster\": {\n",
    "            \"spark_version\": \"13.3.x-scala2.12\",\n",
    "            \"node_type_id\": \"i3.xlarge\",\n",
    "            \"num_workers\": 2,\n",
    "            \"spark_conf\": {\n",
    "                \"spark.databricks.delta.preview.enabled\": \"true\",\n",
    "                \"spark.sql.adaptive.enabled\": \"true\",\n",
    "                \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\"\n",
    "            }\n",
    "        },\n",
    "        \"timeout_seconds\": 3600\n",
    "    },\n",
    "    \n",
    "    \"task_2_silver_transformation\": {\n",
    "        \"task_key\": \"silver_transformation\", \n",
    "        \"description\": \"Transformação Bronze → Silver com limpeza\",\n",
    "        \"depends_on\": [{\"task_key\": \"bronze_ingestion\"}],\n",
    "        \"notebook_task\": {\n",
    "            \"notebook_path\": \"/Workspace/Users/lucaslovatorocha1@gmail.com/stack-tecnico/notebooks/02_bronze_to_silver_etl\",\n",
    "            \"base_parameters\": {\n",
    "                \"environment\": \"production\",\n",
    "                \"date\": \"{{ ds }}\"\n",
    "            }\n",
    "        },\n",
    "        \"new_cluster\": {\n",
    "            \"spark_version\": \"13.3.x-scala2.12\", \n",
    "            \"node_type_id\": \"i3.xlarge\",\n",
    "            \"num_workers\": 4,\n",
    "            \"spark_conf\": {\n",
    "                \"spark.databricks.delta.preview.enabled\": \"true\",\n",
    "                \"spark.sql.adaptive.enabled\": \"true\"\n",
    "            }\n",
    "        },\n",
    "        \"timeout_seconds\": 3600\n",
    "    },\n",
    "    \n",
    "    \"task_3_gold_aggregation\": {\n",
    "        \"task_key\": \"gold_aggregation\",\n",
    "        \"description\": \"Agregações Silver → Gold para análises\",\n",
    "        \"depends_on\": [{\"task_key\": \"silver_transformation\"}], \n",
    "        \"notebook_task\": {\n",
    "            \"notebook_path\": \"/Workspace/Users/lucaslovatorocha1@gmail.com/stack-tecnico/notebooks/03_silver_to_gold_aggregation\",\n",
    "            \"base_parameters\": {\n",
    "                \"environment\": \"production\",\n",
    "                \"date\": \"{{ ds }}\"\n",
    "            }\n",
    "        },\n",
    "        \"new_cluster\": {\n",
    "            \"spark_version\": \"13.3.x-scala2.12\",\n",
    "            \"node_type_id\": \"i3.xlarge\", \n",
    "            \"num_workers\": 3,\n",
    "            \"spark_conf\": {\n",
    "                \"spark.databricks.delta.preview.enabled\": \"true\"\n",
    "            }\n",
    "        },\n",
    "        \"timeout_seconds\": 2400\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\uD83D\uDCCB Tasks do pipeline definidas:\")\n",
    "for task_name, task_config in pipeline_tasks.items():\n",
    "    print(f\"  ✅ {task_config['task_key']}: {task_config['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2b2333-9213-4450-996b-b8ffa6f17061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Configuração de Monitoramento e Alertas\n",
    "# MAGIC \n",
    "# MAGIC Implementando monitoramento robusto para o pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb736879-cfcb-489e-b58e-6262d5c8cbe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA Configurações de monitoramento definidas:\n  \uD83D\uDD0D bronze_data_freshness: Verificar se dados Bronze são atualizados nas últimas 24h\n  \uD83D\uDD0D silver_quality_check: Verificar taxa de retenção Silver > 95%\n  \uD83D\uDD0D gold_completeness: Verificar se agregações Gold foram atualizadas\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# Configurar alertas e monitoramento\n",
    "monitoring_config = {\n",
    "    \"health_checks\": {\n",
    "        \"bronze_data_freshness\": {\n",
    "            \"description\": \"Verificar se dados Bronze são atualizados nas últimas 24h\",\n",
    "            \"query\": f\"\"\"\n",
    "            SELECT \n",
    "                MAX(processed_timestamp) as last_update,\n",
    "                DATEDIFF(HOUR, MAX(processed_timestamp), CURRENT_TIMESTAMP()) as hours_since_update\n",
    "            FROM nyc_taxi_catalog.bronze.raw_data\n",
    "            \"\"\",\n",
    "            \"threshold\": 24,\n",
    "            \"alert_condition\": \"hours_since_update > threshold\"\n",
    "        },\n",
    "        \n",
    "        \"silver_quality_check\": {\n",
    "            \"description\": \"Verificar taxa de retenção Silver > 95%\",\n",
    "            \"query\": f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_silver,\n",
    "                (SELECT COUNT(*) FROM nyc_taxi_catalog.bronze.raw_data) as total_bronze,\n",
    "                (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM nyc_taxi_catalog.bronze.raw_data)) as retention_rate\n",
    "            FROM nyc_taxi_catalog.silver.nyc_taxi_trips\n",
    "            WHERE DATE(processed_timestamp) = CURRENT_DATE()\n",
    "            \"\"\",\n",
    "            \"threshold\": 95.0,\n",
    "            \"alert_condition\": \"retention_rate < threshold\"\n",
    "        },\n",
    "        \n",
    "        \"gold_completeness\": {\n",
    "            \"description\": \"Verificar se agregações Gold foram atualizadas\",\n",
    "            \"query\": f\"\"\"\n",
    "            SELECT COUNT(*) as daily_records\n",
    "            FROM nyc_taxi_catalog.gold.daily_revenue_metrics\n",
    "            WHERE report_date = CURRENT_DATE()\n",
    "            \"\"\",\n",
    "            \"threshold\": 1,\n",
    "            \"alert_condition\": \"daily_records < threshold\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"performance_metrics\": {\n",
    "        \"processing_time\": \"Tempo total de execução do pipeline\",\n",
    "        \"data_volume\": \"Volume de dados processados por camada\",\n",
    "        \"error_rate\": \"Taxa de erro por job\",\n",
    "        \"resource_utilization\": \"Utilização de clusters\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\uD83D\uDCCA Configurações de monitoramento definidas:\")\n",
    "for check_name, check_config in monitoring_config[\"health_checks\"].items():\n",
    "    print(f\"  \uD83D\uDD0D {check_name}: {check_config['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1926f52c-9ad1-45f4-8f4b-a576a45f4e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Workflow JSON gerado com sucesso!\n\uD83D\uDCDD Nome: NYC_Taxi_Pipeline_Production\n\uD83D\uDCC5 Schedule: 0 0 2 * * ?\n\uD83D\uDD27 Tasks: 4\n\uD83D\uDD27 Script de API disponível para automação\n\uD83D\uDCCA Queries de monitoramento configuradas:\n  \uD83D\uDD0D pipeline_health\n  \uD83D\uDD0D data_quality_metrics\n  \uD83D\uDD0D processing_performance\n\uD83E\uDDEA Testando queries de monitoramento...\n\n\uD83D\uDCCB Executando: pipeline_health\n❌ pipeline_health: ERRO - [TABLE_OR_VIEW_NOT_FOUND] The table or view `nyc_taxi_catalog`.`bronze`.`raw_data` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 15 pos 38;\n'Project [pipeline_health AS metric_name#9950, CASE WHEN ((('bronze_count > 0) AND ('silver_count > 0)) AND ('gold_count > 0)) THEN HEALTHY WHEN (('bronze_count > 0) AND ('silver_count > 0)) THEN PARTIAL ELSE UNHEALTHY END AS status#9951, 'bronze_count, 'silver_count, 'gold_count, current_timestamp() AS check_timestamp#9952]\n+- 'SubqueryAlias __auto_generated_subquery_name\n   +- 'Project [scalar-subquery#9944 [] AS bronze_count#9945, scalar-subquery#9946 [] AS silver_count#9947L, scalar-subquery#9948 [] AS gold_count#9949]\n      :  :- 'Aggregate [unresolvedalias(count(1))]\n      :  :  +- 'Filter ('DATE('processed_timestamp) = current_date(Some(Etc/UTC)))\n      :  :     +- 'UnresolvedRelation [nyc_taxi_catalog, bronze, raw_data], [], false\n      :  :- Aggregate [count(1) AS COUNT(*)#10054L]\n      :  :  +- Filter (cast(processed_timestamp#10006 as date) = current_date(Some(Etc/UTC)))\n      :  :     +- SubqueryAlias nyc_taxi_catalog.silver.nyc_taxi_trips\n      :  :        +- Relation nyc_taxi_catalog.silver.nyc_taxi_trips[pickup_datetime#9981,dropoff_datetime#9982,vendor_id#9983,rate_code_id#9984,payment_type#9985,store_and_fwd_flag#9986,passenger_count#9987,trip_distance#9988,pickup_longitude#9989,pickup_latitude#9990,dropoff_longitude#9991,dropoff_latitude#9992,fare_amount#9993,extra#9994,mta_tax#9995,tip_amount#9996,tolls_amount#9997,improvement_surcharge#9998,total_amount#9999,trip_duration_minutes#10000,pickup_hour#10001,pickup_dayofweek#10002,pickup_month#10003,calculated_distance_km#10004,payment_type_desc#10005,... 3 more fields] parquet\n      :  +- 'Aggregate [unresolvedalias(count(1))]\n      :     +- 'Filter ('report_date = current_date(Some(Etc/UTC)))\n      :        +- SubqueryAlias nyc_taxi_catalog.gold.daily_revenue_metrics\n      :           +- Relation nyc_taxi_catalog.gold.daily_revenue_metrics[trip_date#10030,day_of_week#10031,month#10032,year#10033,daily_trips#10034L,daily_passengers#10035L,daily_revenue#10036,daily_fare#10037,daily_tips#10038,avg_trip_value#10039,total_minutes_driven#10040L,total_km_driven#10041,avg_trip_duration#10042,avg_trip_distance#10043,credit_card_trips#10044L,cash_trips#10045L,revenue_per_km#10046,revenue_per_trip#10047,credit_card_percentage#10048,day_type#10049,aggregated_timestamp#10050] parquet\n      +- OneRowRelation\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:345)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:426)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkSubqueryExpression(CheckAnalysis.scala:1143)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkSubqueryExpression$(CheckAnalysis.scala:1138)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkSubqueryExpression(Analyzer.scala:426)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:594)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:474)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:474)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:303)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:302)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:302)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:278)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:426)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:263)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:426)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:254)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:254)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:479)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:479)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:683)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:680)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:631)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:675)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:617)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)\n\n\uD83D\uDCCB Executando: data_quality_metrics\n+------------+------------+------------+-----------------+--------------------+\n| metric_name|quality_flag|record_count|       percentage|     check_timestamp|\n+------------+------------+------------+-----------------+--------------------+\n|data_quality|       valid|    46385364|99.99997844148028|2025-07-31 15:20:...|\n|data_quality|     warning|          10| 0.00002155851972|2025-07-31 15:20:...|\n+------------+------------+------------+-----------------+--------------------+\n\n✅ data_quality_metrics: OK\n\n\uD83D\uDCCB Executando: processing_performance\n+--------------------+------------------+------------------+-----------+--------------------+--------------------+\n|         metric_name| avg_trip_duration|       avg_revenue|total_trips|      last_processed|     check_timestamp|\n+--------------------+------------------+------------------+-----------+--------------------+--------------------+\n|processing_perfor...|14.485172136372125|15.567399442764566|   46385374|2025-07-31 14:22:...|2025-07-31 15:20:...|\n+--------------------+------------------+------------------+-----------+--------------------+--------------------+\n\n✅ processing_performance: OK\n\n\uD83C\uDF89 Configuração de Workflows e Monitoramento Completa!\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Criação do Workflow JSON Completo\n",
    "# MAGIC \n",
    "# MAGIC Gerando configuração completa para Databricks Workflows:\n",
    "\n",
    "# COMMAND ----------\n",
    "# Gerar JSON completo do workflow\n",
    "def create_workflow_json():\n",
    "    \"\"\"\n",
    "    Criar configuração completa do Databricks Workflow\n",
    "    \"\"\"\n",
    "    workflow_json = {\n",
    "        \"name\": workflow_config[\"name\"],\n",
    "        \"description\": workflow_config[\"description\"],\n",
    "        \"max_concurrent_runs\": workflow_config[\"max_concurrent_runs\"],\n",
    "        \"timeout_seconds\": workflow_config[\"timeout_seconds\"],\n",
    "        \n",
    "        \"email_notifications\": {\n",
    "            \"on_start\": workflow_config[\"email_notifications\"][\"on_success\"],\n",
    "            \"on_success\": workflow_config[\"email_notifications\"][\"on_success\"], \n",
    "            \"on_failure\": workflow_config[\"email_notifications\"][\"on_failure\"],\n",
    "            \"no_alert_for_skipped_runs\": False\n",
    "        },\n",
    "        \n",
    "        \"webhook_notifications\": {\n",
    "            \"on_start\": [],\n",
    "            \"on_success\": [],\n",
    "            \"on_failure\": []\n",
    "        },\n",
    "        \n",
    "        \"schedule\": {\n",
    "            \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Diário às 2:00 AM\n",
    "            \"timezone_id\": \"America/Sao_Paulo\",\n",
    "            \"pause_status\": \"UNPAUSED\"\n",
    "        },\n",
    "        \n",
    "        \"tasks\": [\n",
    "            {\n",
    "                \"task_key\": \"bronze_ingestion\",\n",
    "                \"description\": \"Ingestão de dados raw para camada Bronze\",\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Workspace/Users/lucaslovatorocha1@gmail.com/stack-tecnico/notebooks/01_bronze_ingestion\",\n",
    "                    \"base_parameters\": {\n",
    "                        \"environment\": \"production\",\n",
    "                        \"execution_date\": \"{{ ds }}\",\n",
    "                        \"workflow_run_id\": \"{{ run_id }}\"\n",
    "                    }\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                    \"node_type_id\": \"i3.xlarge\",\n",
    "                    \"num_workers\": 2,\n",
    "                    \"spark_conf\": {\n",
    "                        \"spark.databricks.delta.preview.enabled\": \"true\",\n",
    "                        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "                        \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "                        \"spark.sql.adaptive.skewJoin.enabled\": \"true\"\n",
    "                    },\n",
    "                    \"aws_attributes\": {\n",
    "                        \"zone_id\": \"us-west-2a\",\n",
    "                        \"instance_profile_arn\": None,\n",
    "                        \"first_on_demand\": 1,\n",
    "                        \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "                    },\n",
    "                    \"enable_elastic_disk\": True,\n",
    "                    \"disk_spec\": {\n",
    "                        \"disk_type\": {\n",
    "                            \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\"\n",
    "                        },\n",
    "                        \"disk_size\": 100\n",
    "                    }\n",
    "                },\n",
    "                \"timeout_seconds\": 3600,\n",
    "                \"max_retries\": 2,\n",
    "                \"min_retry_interval_millis\": 60000,\n",
    "                \"retry_on_timeout\": True\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"task_key\": \"silver_transformation\",\n",
    "                \"description\": \"Transformação Bronze → Silver com limpeza e validação\",\n",
    "                \"depends_on\": [\n",
    "                    {\n",
    "                        \"task_key\": \"bronze_ingestion\"\n",
    "                    }\n",
    "                ],\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Workspace/Users/lucaslovatorocha1@gmail.com/stack-tecnico/notebooks/02_bronze_to_silver_etl\",\n",
    "                    \"base_parameters\": {\n",
    "                        \"environment\": \"production\",\n",
    "                        \"execution_date\": \"{{ ds }}\",\n",
    "                        \"workflow_run_id\": \"{{ run_id }}\",\n",
    "                        \"source_table\": \"nyc_taxi_catalog.bronze.raw_data\"\n",
    "                    }\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                    \"node_type_id\": \"i3.xlarge\", \n",
    "                    \"num_workers\": 4,\n",
    "                    \"spark_conf\": {\n",
    "                        \"spark.databricks.delta.preview.enabled\": \"true\",\n",
    "                        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "                        \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "                        \"spark.databricks.delta.autoCompact.enabled\": \"true\",\n",
    "                        \"spark.databricks.delta.optimizeWrite.enabled\": \"true\"\n",
    "                    },\n",
    "                    \"aws_attributes\": {\n",
    "                        \"zone_id\": \"us-west-2a\",\n",
    "                        \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "                    },\n",
    "                    \"enable_elastic_disk\": True\n",
    "                },\n",
    "                \"timeout_seconds\": 3600,\n",
    "                \"max_retries\": 2,\n",
    "                \"min_retry_interval_millis\": 120000\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"task_key\": \"gold_aggregation\",\n",
    "                \"description\": \"Agregações Silver → Gold para dashboards analíticos\",\n",
    "                \"depends_on\": [\n",
    "                    {\n",
    "                        \"task_key\": \"silver_transformation\"\n",
    "                    }\n",
    "                ],\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Workspace/Users/lucaslovatorocha1@gmail.com/stack-tecnico/notebooks/03_silver_to_gold_aggregation\",\n",
    "                    \"base_parameters\": {\n",
    "                        \"environment\": \"production\",\n",
    "                        \"execution_date\": \"{{ ds }}\",\n",
    "                        \"workflow_run_id\": \"{{ run_id }}\",\n",
    "                        \"source_table\": \"nyc_taxi_catalog.silver.nyc_taxi_trips\"\n",
    "                    }\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                    \"node_type_id\": \"i3.xlarge\",\n",
    "                    \"num_workers\": 3,\n",
    "                    \"spark_conf\": {\n",
    "                        \"spark.databricks.delta.preview.enabled\": \"true\",\n",
    "                        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "                        \"spark.databricks.delta.autoCompact.enabled\": \"true\"\n",
    "                    },\n",
    "                    \"aws_attributes\": {\n",
    "                        \"zone_id\": \"us-west-2a\",\n",
    "                        \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "                    }\n",
    "                },\n",
    "                \"timeout_seconds\": 2400,\n",
    "                \"max_retries\": 1\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"task_key\": \"data_quality_validation\",\n",
    "                \"description\": \"Validação de qualidade e geração de relatórios\",\n",
    "                \"depends_on\": [\n",
    "                    {\n",
    "                        \"task_key\": \"gold_aggregation\"\n",
    "                    }\n",
    "                ],\n",
    "                \"notebook_task\": {\n",
    "                    \"notebook_path\": \"/Workspace/Users/lucaslovatorocha1@gmail.com/stack-tecnico/notebooks/06_data_quality_validation\",\n",
    "                    \"base_parameters\": {\n",
    "                        \"environment\": \"production\",\n",
    "                        \"execution_date\": \"{{ ds }}\",\n",
    "                        \"workflow_run_id\": \"{{ run_id }}\"\n",
    "                    }\n",
    "                },\n",
    "                \"new_cluster\": {\n",
    "                    \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                    \"node_type_id\": \"i3.large\",\n",
    "                    \"num_workers\": 1,\n",
    "                    \"spark_conf\": {\n",
    "                        \"spark.sql.adaptive.enabled\": \"true\"\n",
    "                    }\n",
    "                },\n",
    "                \"timeout_seconds\": 1200,\n",
    "                \"max_retries\": 1\n",
    "            }\n",
    "        ],\n",
    "        \n",
    "        \"git_source\": {\n",
    "            \"git_url\": \"https://github.com/lucaslovatorocha/nyc-taxi-pipeline\",\n",
    "            \"git_provider\": \"github\",\n",
    "            \"git_branch\": \"main\"\n",
    "        },\n",
    "        \n",
    "        \"run_as\": {\n",
    "            \"service_principal_name\": \"nyc-taxi-pipeline-sp\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return workflow_json\n",
    "\n",
    "# Gerar e exibir configuração\n",
    "workflow_definition = create_workflow_json()\n",
    "print(\"\uD83D\uDE80 Workflow JSON gerado com sucesso!\")\n",
    "print(f\"\uD83D\uDCDD Nome: {workflow_definition['name']}\")\n",
    "print(f\"\uD83D\uDCC5 Schedule: {workflow_definition['schedule']['quartz_cron_expression']}\")\n",
    "print(f\"\uD83D\uDD27 Tasks: {len(workflow_definition['tasks'])}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Salvar configuração em arquivo\n",
    "import json\n",
    "\n",
    "workflow_json_str = json.dumps(workflow_definition, indent=2)\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Script de Criação via API (Opcional)\n",
    "# MAGIC \n",
    "# MAGIC Para automatizar a criação do workflow:\n",
    "\n",
    "# COMMAND ----------\n",
    "# Script para criar workflow via Databricks REST API\n",
    "def create_workflow_via_api(workflow_config, databricks_host, access_token):\n",
    "    \"\"\"\n",
    "    Criar workflow usando Databricks REST API\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    url = f\"{databricks_host}/api/2.1/jobs/create\"\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=workflow_config)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        job_id = response.json()['job_id']\n",
    "        print(f\"✅ Workflow criado com sucesso! Job ID: {job_id}\")\n",
    "        return job_id\n",
    "    else:\n",
    "        print(f\"❌ Erro ao criar workflow: {response.status_code}\")\n",
    "        print(f\"\uD83D\uDCC4 Resposta: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Exemplo de uso (descomente e configure para usar)\n",
    "\"\"\"\n",
    "DATABRICKS_HOST = \"https://your-workspace.cloud.databricks.com\"\n",
    "ACCESS_TOKEN = \"your-access-token\"\n",
    "\n",
    "job_id = create_workflow_via_api(\n",
    "    workflow_definition, \n",
    "    DATABRICKS_HOST, \n",
    "    ACCESS_TOKEN\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\uD83D\uDD27 Script de API disponível para automação\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Monitoramento e Alertas Avançados\n",
    "\n",
    "# COMMAND ----------\n",
    "# Configurar sistema de monitoramento\n",
    "def setup_monitoring_queries():\n",
    "    \"\"\"\n",
    "    Criar queries de monitoramento para o pipeline\n",
    "    \"\"\"\n",
    "    monitoring_queries = {\n",
    "        \"pipeline_health\": \"\"\"\n",
    "        SELECT \n",
    "            'pipeline_health' as metric_name,\n",
    "            CASE \n",
    "                WHEN bronze_count > 0 AND silver_count > 0 AND gold_count > 0 THEN 'HEALTHY'\n",
    "                WHEN bronze_count > 0 AND silver_count > 0 THEN 'PARTIAL'\n",
    "                ELSE 'UNHEALTHY'\n",
    "            END as status,\n",
    "            bronze_count,\n",
    "            silver_count, \n",
    "            gold_count,\n",
    "            CURRENT_TIMESTAMP() as check_timestamp\n",
    "        FROM (\n",
    "            SELECT \n",
    "                (SELECT COUNT(*) FROM nyc_taxi_catalog.bronze.raw_data \n",
    "                 WHERE DATE(processed_timestamp) = CURRENT_DATE()) as bronze_count,\n",
    "                (SELECT COUNT(*) FROM nyc_taxi_catalog.silver.nyc_taxi_trips \n",
    "                 WHERE DATE(processed_timestamp) = CURRENT_DATE()) as silver_count,\n",
    "                (SELECT COUNT(*) FROM nyc_taxi_catalog.gold.daily_revenue_metrics \n",
    "                 WHERE report_date = CURRENT_DATE()) as gold_count\n",
    "        )\n",
    "        \"\"\",\n",
    "        \n",
    "        \"data_quality_metrics\": \"\"\"\n",
    "        SELECT \n",
    "            'data_quality' as metric_name,\n",
    "            quality_flag,\n",
    "            COUNT(*) as record_count,\n",
    "            COUNT(*) * 100.0 / SUM(COUNT(*)) OVER() as percentage,\n",
    "            CURRENT_TIMESTAMP() as check_timestamp\n",
    "        FROM nyc_taxi_catalog.silver.nyc_taxi_trips\n",
    "        WHERE DATE(processed_timestamp) = CURRENT_DATE()\n",
    "        GROUP BY quality_flag\n",
    "        \"\"\",\n",
    "        \n",
    "        \"processing_performance\": \"\"\"\n",
    "        SELECT \n",
    "            'processing_performance' as metric_name,\n",
    "            AVG(trip_duration_minutes) as avg_trip_duration,\n",
    "            AVG(total_amount) as avg_revenue,\n",
    "            COUNT(*) as total_trips,\n",
    "            MAX(processed_timestamp) as last_processed,\n",
    "            CURRENT_TIMESTAMP() as check_timestamp\n",
    "        FROM nyc_taxi_catalog.silver.nyc_taxi_trips\n",
    "        WHERE DATE(processed_timestamp) = CURRENT_DATE()\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    return monitoring_queries\n",
    "\n",
    "monitoring_queries = setup_monitoring_queries()\n",
    "\n",
    "print(\"\uD83D\uDCCA Queries de monitoramento configuradas:\")\n",
    "for query_name in monitoring_queries.keys():\n",
    "    print(f\"  \uD83D\uDD0D {query_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Teste das queries de monitoramento\n",
    "print(\"\uD83E\uDDEA Testando queries de monitoramento...\")\n",
    "\n",
    "for query_name, query_sql in monitoring_queries.items():\n",
    "    try:\n",
    "        print(f\"\\n\uD83D\uDCCB Executando: {query_name}\")\n",
    "        result = spark.sql(query_sql)\n",
    "        result.show(5)\n",
    "        print(f\"✅ {query_name}: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {query_name}: ERRO - {str(e)}\")\n",
    "\n",
    "print(\"\\n\uD83C\uDF89 Configuração de Workflows e Monitoramento Completa!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_databricks_workflows_orchestration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}