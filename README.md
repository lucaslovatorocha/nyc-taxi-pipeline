# NYC Taxi Data Pipeline - Stack Tecnologias
## Desafio TÃ©cnico - Engenheiro de Dados Pleno

![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-0084C7?style=for-the-badge&logo=delta&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)
![AWS S3](https://img.shields.io/badge/AWS%20S3-569A31?style=for-the-badge&logo=amazon-s3&logoColor=white)
![Unity Catalog](https://img.shields.io/badge/Unity%20Catalog-FF3621?style=for-the-badge&logo=databricks&logoColor=white)

## ğŸ“‹ VisÃ£o Geral

Pipeline de dados completo para transformaÃ§Ã£o e anÃ¡lise do dataset NYC Yellow Taxi Trip Data, implementando uma arquitetura moderna de lakehouse com Databricks, Delta Lake e Unity Catalog.

### ğŸ¯ **Objetivos AlcanÃ§ados**
- âœ… **IngestÃ£o**: Dataset carregado para S3 (Bronze Layer)
- âœ… **TransformaÃ§Ã£o**: Pipeline ETL com PySpark (Silver Layer)  
- âœ… **AgregaÃ§Ã£o**: MÃ©tricas analÃ­ticas otimizadas (Gold Layer)
- âœ… **Warehouse**: Esquema estrela para consultas analÃ­ticas
- âœ… **OrquestraÃ§Ã£o**: Databricks Workflows automatizado
- âœ… **GovernanÃ§a**: Unity Catalog com seguranÃ§a e catalogaÃ§Ã£o

### ğŸ“Š **MÃ©tricas do Pipeline**
- **46.4 milhÃµes** de registros processados
- **98.17%** taxa de retenÃ§Ã£o de dados
- **541K** agregaÃ§Ãµes horÃ¡rias por localizaÃ§Ã£o
- **122 dias** de dados histÃ³ricos (Jan 2015 - Mar 2016)
- **Performance sub-segundo** para consultas analÃ­ticas

## ğŸ—ï¸ Arquitetura

### **Arquitetura Lakehouse Implementada**
```
ğŸ“¥ Kaggle Dataset (NYC Taxi)
    â†“
ğŸ¥‰ Bronze Layer (S3) - Dados Raw
    â†“ PySpark ETL
ğŸ¥ˆ Silver Layer (S3) - Dados Limpos  
    â†“ AgregaÃ§Ãµes
ğŸ¥‡ Gold Layer (S3) - MÃ©tricas AnalÃ­ticas
    â†“ Schema Estrela
ğŸ¢ Warehouse (Delta) - Consultas Otimizadas
```

### **Stack TecnolÃ³gico**
- **Plataforma**: Databricks Workspace
- **Storage**: AWS S3 (us-west-2)
- **Processamento**: Apache Spark + PySpark
- **Formato**: Delta Lake
- **GovernanÃ§a**: Unity Catalog
- **OrquestraÃ§Ã£o**: Databricks Workflows
- **Warehouse**: Databricks SQL Warehouse

## ğŸš€ InÃ­cio RÃ¡pido

### **PrÃ©-requisitos**
- Conta AWS com acesso a S3 e IAM
- Databricks Workspace (Premium/Enterprise)
- Python 3.8+
- Kaggle API configurada

### **1. ConfiguraÃ§Ã£o da Infraestrutura**
```bash
# Criar buckets S3
aws s3api create-bucket --bucket nyc-taxi-bronze-lucas --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2
aws s3api create-bucket --bucket nyc-taxi-silver-lucas --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2
aws s3api create-bucket --bucket nyc-taxi-gold-lucas --region us-west-2 --create-bucket-configuration LocationConstraint=us-west-2

# Configurar Unity Catalog (ver docs/setup.md)
```

### **2. ExecuÃ§Ã£o do Pipeline**
```python
# 1. Executar notebooks na ordem:
notebooks/01_bronze_ingestion.py
notebooks/02_bronze_to_silver_etl.py  
notebooks/03_silver_to_gold_aggregation.py
notebooks/04_sql_warehouse_setup.py

# 2. Ou usar Databricks Workflows (recomendado)
# Ver: workflows/nyc_taxi_pipeline.json
```

## ğŸ“ Estrutura do Projeto

```
nyc-taxi-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md          # Arquitetura detalhada
â”‚   â”œâ”€â”€ setup.md                # Guia de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ data-modeling.md         # Modelagem de dados
â”‚   â””â”€â”€ performance.md           # OtimizaÃ§Ãµes aplicadas
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_bronze_ingestion.py
â”‚   â”œâ”€â”€ 02_bronze_to_silver_etl.py
â”‚   â”œâ”€â”€ 03_silver_to_gold_aggregation.py
â”‚   â””â”€â”€ 04_sql_warehouse_setup.py
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ nyc_taxi_pipeline.json   # DefiniÃ§Ã£o do workflow
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_schemas.sql
â”‚   â”œâ”€â”€ validation_queries.sql
â”‚   â””â”€â”€ analytics_views.sql
â”œâ”€â”€ config/
â”‚   â””â”€â”€ unity_catalog_setup.sql
â””â”€â”€ evidence/
    â”œâ”€â”€ screenshots/
    â”œâ”€â”€ performance_metrics.md
    â””â”€â”€ data_quality_report.md
```

## ğŸ” Resultados e EvidÃªncias

### **Qualidade dos Dados**
- **Taxa de RetenÃ§Ã£o**: 98.17% (46.4M de 47.2M registros)
- **Dados VÃ¡lidos**: 99.9999% (apenas 10 registros com warnings)
- **ConsistÃªncia Temporal**: 99.89% timestamps vÃ¡lidos
- **Coordenadas NYC**: 98.19% dentro dos limites geogrÃ¡ficos

### **Performance do Pipeline**
- **IngestÃ£o Bronze**: ~5 minutos para dataset completo
- **TransformaÃ§Ã£o Silver**: ~15 minutos (46M registros)
- **AgregaÃ§Ã£o Gold**: ~8 minutos (541K mÃ©tricas)
- **Consultas AnalÃ­ticas**: <1 segundo (mÃ©dia)

### **MÃ©tricas de NegÃ³cio**
- **Receita Total**: $722 milhÃµes processados
- **Viagens DiÃ¡rias**: 380K em mÃ©dia
- **Valor MÃ©dio**: $15.57 por viagem
- **DuraÃ§Ã£o MÃ©dia**: 14.5 minutos
- **DistÃ¢ncia MÃ©dia**: 3.33 km

## ğŸ”’ SeguranÃ§a e GovernanÃ§a

### **ImplementaÃ§Ãµes de SeguranÃ§a**
- âœ… **Criptografia**: S3 SSE-AES256 em repouso
- âœ… **IAM**: PrincÃ­pio do menor privilÃ©gio
- âœ… **Unity Catalog**: Controle de acesso granular
- âœ… **External Locations**: Acesso controlado ao S3
- âœ… **Service Principals**: AutenticaÃ§Ã£o automatizada

### **CatalogaÃ§Ã£o de Dados**
- **CatÃ¡logo**: `nyc_taxi_catalog`
- **Schemas**: `bronze`, `silver`, `gold`, `warehouse`
- **Lineage**: Rastreamento automÃ¡tico Delta Lake
- **Metadados**: DocumentaÃ§Ã£o completa no Unity Catalog

## ğŸ“ˆ Consultas AnalÃ­ticas

### **Exemplos de Queries**
```sql
-- Top 5 meses por receita (2015)
SELECT 
    month_name,
    monthly_trips,
    ROUND(monthly_revenue, 0) as revenue
FROM nyc_taxi_catalog.warehouse.vw_executive_dashboard
WHERE year = 2015
ORDER BY monthly_revenue DESC
LIMIT 5;

-- AnÃ¡lise por tipo de pagamento
SELECT 
    payment_type_desc,
    COUNT(*) as trips,
    ROUND(AVG(total_amount), 2) as avg_fare
FROM nyc_taxi_catalog.silver.nyc_taxi_trips
GROUP BY payment_type_desc
ORDER BY trips DESC;
```

## ğŸ”§ ConfiguraÃ§Ã£o e Deploy

### **VariÃ¡veis de Ambiente**
```bash
# AWS
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
AWS_DEFAULT_REGION=us-west-2

# Databricks
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=your_token

# Kaggle
KAGGLE_USERNAME=your_username
KAGGLE_KEY=your_key
```

### **ConfiguraÃ§Ã£o do Unity Catalog**
Ver documentaÃ§Ã£o detalhada em [`docs/setup.md`](docs/setup.md)

## ğŸ§ª Testes e ValidaÃ§Ã£o

### **Testes Implementados**
- âœ… **Integridade**: Contagem de registros por camada
- âœ… **Qualidade**: ValidaÃ§Ã£o de valores nulos e outliers
- âœ… **Performance**: Benchmarks de consultas
- âœ… **ConsistÃªncia**: VerificaÃ§Ã£o de agregaÃ§Ãµes

### **Executar Testes**
```python
# Ver notebooks de validaÃ§Ã£o
notebooks/05_data_validation.py
```

## ğŸ“Š Monitoramento

### **MÃ©tricas Monitoradas**
- Taxa de retenÃ§Ã£o de dados por camada
- Tempo de execuÃ§Ã£o do pipeline
- Qualidade dos dados processados
- Performance das consultas analÃ­ticas

### **Alertas Configurados**
- Falhas no pipeline (email)
- DegradaÃ§Ã£o de performance
- Problemas de qualidade de dados

## ğŸ¤ ContribuiÃ§Ã£o

### **PrÃ³ximas Melhorias**
- [ ] ImplementaÃ§Ã£o de Delta Live Tables
- [ ] IntegraÃ§Ã£o com Tableau/Power BI
- [ ] OtimizaÃ§Ã£o com Z-ORDER automÃ¡tico
- [ ] ImplementaÃ§Ã£o de CDC (Change Data Capture)
- [ ] ExpansÃ£o para outros datasets NYC

## ğŸ‘¨â€ğŸ’» Autor

**Lucas Lovato**
- ğŸ“§ Email: lucaslovatotech@gmail.com
- ğŸ’¼ LinkedIn: [lucas-lovato](https://linkedin.com/in/lucas-lovato)
- ğŸ™ GitHub: [lucaslovato](https://github.com/lucaslovato)

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como parte do desafio tÃ©cnico para Stack Tecnologias.

---

## ğŸ† **ConclusÃ£o**

Pipeline completo de dados implementado com sucesso, demonstrando:
- **Arquitetura moderna** de lakehouse
- **Processamento escalÃ¡vel** com Spark
- **GovernanÃ§a robusta** com Unity Catalog  
- **Performance otimizada** para anÃ¡lises
- **SeguranÃ§a enterprise** com AWS + Databricks

**Status**: âœ… **ProduÃ§Ã£o Ready**