# Arquitetura do Pipeline NYC Taxi

## üèóÔ∏è Vis√£o Geral da Arquitetura

### **Arquitetura Lakehouse Implementada**

```mermaid
graph TD
    A[Kaggle Dataset] --> B[S3 Bronze Layer]
    B --> C[Databricks PySpark ETL]
    C --> D[S3 Silver Layer]
    D --> E[Databricks Aggregation]
    E --> F[S3 Gold Layer]
    F --> G[Databricks SQL Warehouse]
    
    H[Unity Catalog] --> B
    H --> D
    H --> F
    H --> G
    
    I[Databricks Workflows] --> C
    I --> E
```

## üìä Camadas de Dados

### **Bronze Layer (Raw Data)**
- **Localiza√ß√£o**: `s3://nyc-taxi-bronze-lucas/raw/`
- **Formato**: CSV (dados originais do Kaggle)
- **Prop√≥sito**: Armazenamento de dados brutos sem transforma√ß√£o
- **Volume**: ~47.2M registros originais
- **Schema**: Preserva estrutura original (strings)

### **Silver Layer (Clean Data)**
- **Localiza√ß√£o**: `s3://nyc-taxi-silver-lucas/processed/`
- **Formato**: Delta Lake
- **Prop√≥sito**: Dados limpos, validados e tipados
- **Volume**: 46.4M registros (98.17% reten√ß√£o)
- **Transforma√ß√µes**:
  - Convers√£o de tipos de dados
  - Filtros de qualidade
  - Enriquecimento com campos derivados
  - Valida√ß√£o de coordenadas NYC

### **Gold Layer (Analytics Data)**
- **Localiza√ß√£o**: `s3://nyc-taxi-gold-lucas/analytics/`
- **Formato**: Delta Lake
- **Prop√≥sito**: Agrega√ß√µes e m√©tricas para an√°lises
- **Tabelas**:
  - `hourly_location_metrics`: 541K registros
  - `daily_revenue_metrics`: 122 registros
  - `executive_kpis`: 1 registro consolidado

### **Warehouse Layer (Dimensional Model)**
- **Localiza√ß√£o**: Databricks Managed Tables
- **Formato**: Delta Lake
- **Prop√≥sito**: Esquema estrela otimizado para consultas anal√≠ticas
- **Estrutura**:
  - `fact_taxi_trips`: Tabela fato principal
  - `dim_time`: Dimens√£o temporal
  - `dim_location`: Dimens√£o geogr√°fica
  - `dim_payment`: Dimens√£o de pagamento

## üîß Componentes T√©cnicos

### **Databricks Workspace**
- **Regi√£o**: us-west-2
- **Tier**: Premium (Unity Catalog habilitado)
- **Clusters**: Auto-scaling com SPOT instances
- **Runtime**: 13.3.x-scala2.12

### **Unity Catalog**
- **Metastore**: `nyc-taxi-metastore`
- **Cat√°logo**: `nyc_taxi_catalog`
- **Schemas**: `bronze`, `silver`, `gold`, `warehouse`
- **External Locations**: Configuradas para todos os buckets S3

### **AWS S3 Configuration**
```json
{
  "buckets": {
    "bronze": "nyc-taxi-bronze-lucas",
    "silver": "nyc-taxi-silver-lucas", 
    "gold": "nyc-taxi-gold-lucas",
    "managed": "nyc-taxi-managed-lucaslovato"
  },
  "region": "us-west-2",
  "encryption": "SSE-AES256",
  "versioning": "Enabled"
}
```

### **IAM Configuration**
```json
{
  "role": "databricks-unity-catalog-role",
  "policies": [
    {
      "name": "S3AccessPolicy",
      "permissions": ["s3:GetObject", "s3:PutObject", "s3:DeleteObject"],
      "resources": ["arn:aws:s3:::nyc-taxi-*/*"]
    }
  ]
}
```

## üîÑ Pipeline de Transforma√ß√£o

### **ETL Process Flow**

#### **1. Bronze ‚Üí Silver (Data Cleaning)**
```python
# Principais transforma√ß√µes
- Convers√£o de tipos (string ‚Üí timestamp, double, int)
- Filtros de qualidade (valores nulos, negativos, outliers)
- Valida√ß√£o geogr√°fica (coordenadas NYC)
- Enriquecimento temporal (hora, dia da semana, m√™s)
- C√°lculos derivados (dura√ß√£o, dist√¢ncia)
- Classifica√ß√£o de qualidade (valid, warning, error)
```

**Crit√©rios de Qualidade Silver:**
- Timestamps v√°lidos e consistentes
- Coordenadas dentro dos limites de NYC
- Valores monet√°rios positivos
- Dist√¢ncias e dura√ß√µes realistas
- Contagem de passageiros v√°lida (1-6)

#### **2. Silver ‚Üí Gold (Aggregations)**
```python
# Agrega√ß√µes implementadas
- M√©tricas hor√°rias por localiza√ß√£o
- Resumos di√°rios de receita
- KPIs executivos consolidados
- An√°lises de tend√™ncias temporais
```

#### **3. Gold ‚Üí Warehouse (Dimensional Model)**
```python
# Esquema estrela
- Tabela fato com chaves dimensionais
- Dimens√µes de tempo, localiza√ß√£o e pagamento
- Views anal√≠ticas otimizadas
- Particionamento por data
```

## ‚ö° Otimiza√ß√µes de Performance

### **Delta Lake Optimizations**
- **Auto Compaction**: Habilitado em todas as camadas
- **Optimize Write**: Reduz small files
- **Partitioning**: Por data nas tabelas principais
- **Z-Ordering**: Em colunas de consulta frequente

### **Spark Configurations**
```python
spark_configs = {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    "spark.databricks.delta.autoCompact.enabled": "true",
    "spark.databricks.delta.optimizeWrite.enabled": "true"
}
```

### **Cluster Optimization**
- **Node Type**: i3.xlarge (memory optimized)
- **Auto Scaling**: 2-8 workers conforme demanda
- **Spot Instances**: 80% economia de custos
- **Disk**: SSD com elastic disk habilitado

## üîí Seguran√ßa e Governan√ßa

### **Access Control**
- **Unity Catalog**: Controle granular por schema/tabela
- **Service Principals**: Autentica√ß√£o para jobs automatizados
- **IAM Roles**: Princ√≠pio do menor privil√©gio
- **Network Security**: VPC endpoints (quando necess√°rio)

### **Data Lineage**
- **Automatic Tracking**: Delta Lake rastreia origem dos dados
- **Unity Catalog**: Visualiza√ß√£o completa da lineage
- **Audit Logs**: Todas as opera√ß√µes s√£o registradas

### **Encryption**
- **At Rest**: S3 SSE-AES256
- **In Transit**: HTTPS/TLS para todas as comunica√ß√µes
- **Databricks**: Encryption at rest habilitado

## üìà Monitoramento e Observabilidade

### **M√©tricas Coletadas**
- Volume de dados por camada
- Tempo de execu√ß√£o por job
- Taxa de erro e reprocessamento
- Performance de queries
- Utiliza√ß√£o de recursos

### **Alertas Configurados**
- Pipeline failures ‚Üí Email notification
- Data quality degradation ‚Üí Slack/Email
- Performance regression ‚Üí Dashboard alerts
- Resource utilization ‚Üí Cost optimization alerts

## üîÑ Disaster Recovery

### **Backup Strategy**
- **S3 Versioning**: Habilitado em todos os buckets
- **Cross-Region Replication**: Para dados cr√≠ticos
- **Delta Lake Time Travel**: 30 dias de hist√≥rico
- **Unity Catalog Backup**: Metadados replicados

### **Recovery Procedures**
1. **Data Recovery**: Time travel ou S3 versioning
2. **Infrastructure**: Terraform para rebuild
3. **Catalog Recovery**: Export/import de metadados
4. **Testing**: Recovery procedures testados mensalmente

## üöÄ Escalabilidade

### **Horizontal Scaling**
- **Auto-scaling clusters**: Baseado na carga de trabalho
- **Parallel processing**: Particionamento otimizado
- **Resource pools**: Isolamento de workloads

### **Vertical Scaling**
- **Memory optimization**: Para datasets maiores
- **Compute optimization**: GPU clusters quando necess√°rio
- **Storage optimization**: Tiering autom√°tico S3

## üìä Performance Benchmarks

### **Processing Times**
| Stage | Data Volume | Processing Time | Throughput |
|-------|-------------|-----------------|------------|
| Bronze Ingestion | 47.2M records | ~5 minutes | 157K records/sec |
| Silver Transformation | 46.4M records | ~15 minutes | 51K records/sec |
| Gold Aggregation | 541K metrics | ~8 minutes | 1K metrics/sec |
| Warehouse Load | 46.4M records | ~12 minutes | 64K records/sec |

### **Query Performance**
| Query Type | Avg Response Time | P95 Response Time |
|------------|-------------------|-------------------|
| Simple Filter | 0.5s | 1.2s |
| Aggregation | 0.8s | 2.1s |
| Join (2 tables) | 1.2s | 3.5s |
| Complex Analytics | 2.1s | 5.8s |

## üîÆ Pr√≥ximas Evolu√ß√µes

### **Curto Prazo (1-3 meses)**
- Implementa√ß√£o de Delta Live Tables
- Otimiza√ß√£o autom√°tica com AI/ML
- Integra√ß√£o com ferramentas de BI

### **M√©dio Prazo (3-6 meses)**
- Real-time streaming com Kafka
- Machine Learning pipelines
- Advanced analytics com MLflow

### **Longo Prazo (6+ meses)**
- Multi-cloud deployment
- Advanced governance com Purview
- Automated data discovery